<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Project page for LoRA-A²">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>

  <meta property="og:title" content="LoRA-A²"/>
  <meta property="og:description" content="Towards Robust and Efficient Federated Low-Rank Adaptation with Heterogeneous Clients"/>
  <meta property="og:url" content="https://pseudope.github.io/LoRA-A2/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/main_figure_final.png"/>
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="LoRA-A²">
  <meta name="twitter:description" content="Towards Robust and Efficient Federated Low-Rank Adaptation with Heterogeneous Clients">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/main_figure_final.png">
  <meta name="twitter:card" content="static/images/main_figure_final.png">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Federated Learning, FL, Low-Rank Adaptation, LoRA, ACL, ACL 2025">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>LoRA-A²</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>

  <style>
    .logo-bar {
      display: flex;
      justify-content: space-between;
      align-items: center;
      padding: 10px 40px;
      flex-wrap: nowrap;
      max-width: 100%;
      overflow-x: auto;
      gap: 1rem;
    }
  
    .logo-bar img {
      max-height: 60px;
      height: auto;
      width: auto;
      max-width: 45%;
      object-fit: contain;
    }
  
    .logo-left {
      transform: scale(1.25);
      transform-origin: left center;
    }
  
    .logo-right {
      transform: scale(0.75);
      transform-origin: right center;
    }
  
    @media (max-width: 768px) {
      .logo-bar {
        flex-wrap: nowrap;
        padding: 10px 20px;
        gap: 0.5rem;
      }
  
      .logo-bar img {
        max-width: 40%;
        margin: 0;
        width: clamp(80px, 20vw, 180px);
      }
  
      .logo-left,
      .logo-right {
        transform: none !important;
      }
    }

    .prop-box {
      border: 1px solid #ccc;
      border-radius: 8px;
      background-color: #f9f9f9;
      padding: 1.2em;
      margin: 2em auto;
      max-width: 900px;
      display: flex;
      overflow-x: auto;
      -webkit-overflow-scrolling: touch;
    }

    .prop-label {
      flex: 0 0 140px;
      font-weight: bold;
      font-size: 1.05em;
      padding-right: 1em;
      text-align: left;
    }

    .prop-content {
      flex: 1;
      text-align: justify;
    }

    .prop-content p {
      margin-bottom: 0.75em;
    }

    .prop-content .math {
      text-align: center;
      margin-top: 1em;
    }

    .responsive-table {
      overflow-x: auto;
      border: 1px solid #ccc;
      border-radius: 8px;
      margin: 2em 0;
      width: 100%;
      -webkit-overflow-scrolling: touch;
    }

    table.lora-table {
      width: 100%;
      table-layout: auto !important;
      border-collapse: collapse;
      font-size: 0.9em;
      min-width: 900px;
    }

    table.lora-table th, table.lora-table td {
      padding: 0.6em;
      border: 1px solid #ccc;
      text-align: center !important;
      vertical-align: middle !important;
      line-height: 1.2em;
      white-space: normal;
    }

    table.lora-table th {
      background-color: #f7f7f7;
      font-weight: bold;
    }

    .bold { font-weight: bold; }
    .underline { text-decoration: underline; }

    .lora-table td .pm {
      display: inline-block;         /* ← 핵심! */
      font-size: 0.8em;
      font-weight: normal;
      text-decoration: none;
      vertical-align: baseline;
    }

    .lora-table td.underline .pm {
      text-decoration: none !important;
    }
  </style>
  
</head>
<body>

  <div class="logo-bar">
    <img src="static/images/mllab_logo.png" alt="MLLab Logo" class="logo-left">
    <img src="static/images/acl_logo.png" alt="ACL Logo" class="logo-right">
  </div>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title">
              Towards Robust and Efficient Federated<br>
              Low-Rank Adaptation with Heterogeneous Clients
            </h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Jabin Koo</a><sup>*1</sup>,</span>
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Minwoo Jang</a><sup>*2</sup>,</span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Jungseul Ok</a><sup>†12</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">{<sup>1</sup> Department of Computer Science and Engineering, <sup>2</sup> Graduate School of Artificial Intelligence}<br>
                      POSTECH, South Korea
                    </span>
                    
                    <span class="eql-cntrb"><small><br><sup>*</sup>Equal Contribution &nbsp; <sup>†</sup>Corresponding Author</small></span><br>
                    
                    <span class="publication-venue">ACL 2025 (Main, Long)</span>

                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2410.22815" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/pseudope/LoRA-A2" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (to be released soon)</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <figure class="image">
        <img src="./static/images/main_figure_final.png" alt="Teaser image", style="max-width: 100%; height: auto;">
      </figure>
      <h2 class="subtitle has-text-centered" style="margin-top: 1.5rem;">
        <strong>LoRA-A<sup>2</sup></strong> comprises two key components: <strong>Alternating freeze</strong> and <strong>Adaptive rank selection</strong>.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser image -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Federated fine-tuning for Large Language Models (LLMs) has recently gained attention due to the heavy communication overhead of transmitting large model updates.
            Low Rank Adaptation (LoRA) has been proposed as a solution, yet its application in federated learning is complicated by discordance in aggregation.
            Existing methods addressing this discordance often suffer from performance degradation at low ranks in heterogeneous data settings.
            In response, we introduce LoRA-A<sup>2</sup> (<b>Lo</b>w <b>R</b>ank <b>A</b>daptation with <b>A</b>lternating freeze and <b>A</b>daptive rank selection),
            which demonstrates robustness in challenging settings with low ranks and high data heterogeneity.
            Our experimental findings reveal that LoRA-A<sup>2</sup> maintains performance even under extreme heterogeneity and low rank conditions,
            achieving up to a 99.8% reduction in uploaded parameters compared to full fine-tuning without compromising performance.
            This adaptive mechanism boosts robustness and communication efficiency in federated fine-tuning,
            enabling the practical deployment of LLMs in resource-constrained environments.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Paper introduction -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Discordance Problem in Federated LoRA</h2>
        <div class="content has-text-justified">
          <p>
            LoRA approximates the fine-tuned weight as \( W = W_0 + \Delta W = W_0 + BA \), where \( W_0 \) is the frozen pre-trained matrix,
            and \( B \in \mathbb{R}^{d_1 \times r}, A \in \mathbb{R}^{r \times d_2} \) are low-rank matrices with rank \( r \ll \min(d_1, d_2) \).
            This reduces the number of trainable parameters from \( d_1 \cdot d_2 \) to \( r \cdot (d_1 + d_2) \),
            which benefits in both <strong>communication and computation efficiency</strong> in Federated Learning (FL).
          </p>
        
          <p>
            However, due to <strong>the bilinear parametrization of LoRA</strong>,
            aggregating client updates \( ( \Delta W_k = B_kA_k \; \forall k \in [K] )\) leads to <strong>the discordance problem</strong>:
          </p>
        
          <p>
            <div style="overflow-x: auto; max-width: 100%;">
              \[
              \sum_{k=1}^K w_k B_k A_k \neq \left( \sum_{k=1}^K w_k B_k \right) \left( \sum_{k=1}^K w_k A_k \right),
              \]
            </div>
          </p>
        
          <p>
            where \( w_k \) are non-negative aggregation weights with \( \sum w_k = 1 \).
            This mismatch degrades performance, which has led to a growing body of research aimed at addressing this issue.
          </p>

          <ul>
            <li><strong>FFA-LoRA [Sun et al.]</strong> Let \( A_k = A_0 \) be frozen for all \( k \in [K] \), so that
              <div style="overflow-x: auto; max-width: 100%;">
                \[
                \sum_{k=1}^K w_k B_k A_k = \left( \sum_{k=1}^K w_k B_k \right) A_0.
                \]
              </div>
            </li>
            <li><strong>FlexLoRA [Bai et al.]</strong> Aggregate \( B_kA_k \) directly. Then, using SVD, decompose it back into
              <div style="overflow-x: auto; max-width: 100%;">
                \[
                \sum_{k=1}^K w_k B_k A_k = BA.
                \]
              </div>
            </li>
          </ul>

        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Limited Parameter Space in LoRA</h2>

    <!-- 그림 2개 나란히 -->
    <div class="columns is-vcentered">
      
      <!-- 왼쪽 그림 + 개별 캡션 -->
      <div class="column has-text-centered">
        <figure class="image is-4by3">
          <img src="static/images/conflict_16_01.png" alt="Method A">
        </figure>
        <p class="caption">(a) \( Dir(0.1) \)</p>
      </div>

      <!-- 오른쪽 그림 + 개별 캡션 -->
      <div class="column has-text-centered">
        <figure class="image is-4by3">
          <img src="static/images/conflict_16_001.png" alt="Method B">
        </figure>
        <p class="caption">(b) \( Dir(0.01) \)</p>
      </div>

    </div>

    <!-- 공통 캡션 -->
    <p class="has-text-centered caption" style="margin-top: 1.5rem;">
      <strong>Figure 1:</strong> Accuracy of previous Federated LoRA methods across different rank sizes in heterogeneous data settings.
    </p>

    <!-- <div class="content" style="margin-top: 1.5rem;">
      <p>
        Especially with low rank and high data heterogeneity, existing methods struggle to maintain performance.
      </p>
    </div> -->
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          <div class="content" style="margin-top: 1.5rem;">
            <p>
              However, as shown in Figure 1, existing methods struggle to maintain performance, especially with low rank and high data heterogeneity.
              One possible explanation is that <strong> the parameter space available to these methods might be insufficient </strong> to accommodate diverse local updates.
              Our goal is to address the discordance problem while maintaining performance in such challenging settings.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Alternating Freeze</h2>
        <div class="content has-text-justified">
          
          <p>
            Instead of solely training module \( B \) while keeping module \( A \) frozen permanently,
            LoRA-A<sup>2</sup> alternates between the two:
            LoRA module \( A \) is frozen during even rounds,
            while module \( B \) is frozen during odd rounds.
            This method preserves the optimization space while effectively resolving discordance.
            Specifically, when freezing \( A \), we have
            <div style="overflow-x: auto; max-width: 100%;">
              \[
              \Delta W = \sum_{k=1}^K \left( w_k B_k \right) A = \sum_{k=1}^K \left( w_k B_k A \right) = \sum_{k=1}^K \left( w_k B_k A_k \right) = \sum_{k=1}^K \left( w_k \Delta W_k \right),
              \]
            </div>
            and when freezing \( B \), we have
            <div style="overflow-x: auto; max-width: 100%;">
              \[
              \Delta W = \sum_{k=1}^K B \left( w_k A_k \right) = \sum_{k=1}^K \left( w_k B A_k \right) = \sum_{k=1}^K \left( w_k B_k A_k \right) = \sum_{k=1}^K \left( w_k \Delta W_k \right).
              \]
            </div>
            In this way, <strong>LoRA-A<sup>2</sup> trains both \( B \) and \( A \)</strong>, ensuring that \( A \) does not remain the same as its initial value.
            (Note: Since the standard convention of LoRA initializes \( B \) to zero, we first train \( B \).
            However, if a different initialization scheme is used, training \( A \) first could also be a reasonable choice.)
          </p>
          
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Adaptive Rank Selection</h2>
        <div class="content has-text-justified">
          
          <p>
            LoRA-A<sup>2</sup> selects important LoRA ranks to match local communication rank budget \( r_i \)
            out of global LoRA adapter with rank \( r_G \) <strong>adaptively</strong> based on the local dataset.
            This approach provides two key benefits:
          </p>
          <ul>
            <li>It minimizes client conflicts by allowing <strong>different clients to choose different LoRA ranks in high heterogeneity</strong>.</li>
            <li>
              It <strong>reallocates rank resources</strong> from unimportant LoRA modules to modules that require more fine-tuning,
              which is <strong>especially effective when communication rank budget is small</strong>.
            </li>
          </ul>
          <p>
            Figure 2 below shows the number of selected ranks per module for three clients in a pathological data distribution setting.
            In this setup, clients 0 and 1 are assigned the "medical" and "space" classes respectively,
            while client 2 is assigned "motorcycle" and "religions" classes from the 20 Newsgroups dataset.
            This indicates that <strong>clients with semantically related data tend to converge on similar rank subspaces, potentially enabling more effective cooperative training</strong>.
            Meanwhile, clients with divergent data distributions select distinct ranks, leading to more independent updates.
            In addition, we observe that most modules are assigned zero ranks across clients.
            This suggests that <strong>our adaptive mechanism effectively prunes out modules that do not benefit from fine-tuning</strong>.
            Please refer to Sections 4.2 and 4.3 for the detailed methodology of our rank selection mechanism.
          </p>

          <!-- 그림 3개 나란히 -->
          <div class="columns is-vcentered is-multiline">

            <!-- 왼쪽 그림 -->
            <div class="column is-one-third has-text-centered">
              <figure class="image">
                <img src="static/images/0_module_selection.png" alt="ars_client_0">
              </figure>
              <p class="caption">(a) Client 0</p>
            </div>

            <!-- 가운데 그림 -->
            <div class="column is-one-third has-text-centered">
              <figure class="image">
                <img src="static/images/1_module_selection.png" alt="ars_client_1">
              </figure>
              <p class="caption">(b)  Client 1</p>
            </div>

            <!-- 오른쪽 그림 -->
            <div class="column is-one-third has-text-centered">
              <figure class="image">
                <img src="static/images/2_module_selection.png" alt="ars_client_2">
              </figure>
              <p class="caption">(c)  Client 2</p>
            </div>

          </div>

          <!-- 공통 캡션 -->
          <p class="caption" style="margin-top: 1.5rem; text-align: justify; width: 100%;">
            <strong>Figure 2:</strong> Visualization on number of selected rank per module.
            The x-axis shows RoBERTa module types, while the y-axis indicates layer numbers.
            Experimented on the 20 Newsgroups dataset with a pathological data distribution.
            Average 2 ranks were selected out of 16 ranks by our adaptive rank selection algorithm.
          </p>
          


          <!-- <div class="prop-box">
            <div class="prop-label">Proposition 1.</div>
            <div class="prop-content" style="text-align: justify;">
              <p>
                For a model \( W \), consider LoRA-based FL algorithms which update \( r \) rank parameters per round.
                Let \( \Omega_{\mathcal{A}} \) denote the space of all possible parameter values
                that an algorithm in \( \mathcal{A} \) can make, where:
              </p>
              <div class="math" style="text-align: center;">
                \[
                  \mathcal{A} \in \left\{ \text{FFA-LoRA}, \text{FL+LoRA}, \text{FlexLoRA}, \text{LoRA-A}^2 \right\}.
                \]
              </div>
              <p>
                Then, we have:
              </p>
              <div class="math" style="text-align: center;">
                \[
                  \Omega_{\text{FFA-LoRA}} \subsetneq \Omega_{\text{FL+LoRA}} = \Omega_{\text{FlexLoRA}} \subset \Omega_{\text{LoRA-A}^2}.
                \]
              </div>
            </div>
          </div> -->
          
          
          
          
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Experimental Results</h2>
        <div class="content has-text-justified">
          
          <div class="responsive-table">
            <style>
              .oracle-highlight {
                background-color: #fdf6e3;
              }
              .lora-table td.rotate-label:hover {
                background-color: #e0f7fa;
                cursor: pointer;
              }
                .lora-table tr td:hover {
                background-color: #e0f7fa;
              }
            </style>
            
            <table class="lora-table">
              <thead>
                <tr>
                  <th rowspan="2">LoRA Rank</th>
                  <th rowspan="2">Method</th>
                  <th colspan="3">BANKING77 Dataset</th>
                  <th colspan="3">20 Newsgroups Dataset</th>
                  <th rowspan="2">Communicated<br>Parameters<sup>*</sup></th>
                </tr>
                <tr>
                  <th>Dir(0.5)</th><th>Dir(0.1)</th><th>Dir(0.01)</th>
                  <th>Dir(0.5)</th><th>Dir(0.1)</th><th>Dir(0.01)</th>
                </tr>
              </thead>
              <tbody>
                <tr class="oracle-highlight">
                  <td rowspan="1" class="rotate-label">w/o LoRA</td>
                  <td>FL (w/o LoRA)</td>
                  <td>92.76 <span class="pm">± 0.30</span></td>
                  <td>90.29 <span class="pm">± 0.73</span></td>
                  <td>67.58 <span class="pm">± 0.44</span></td>
                  <td>70.93 <span class="pm">± 1.04</span></td>
                  <td>68.82 <span class="pm">± 0.69</span></td>
                  <td>64.41 <span class="pm">± 0.30</span></td>
                  <td>186B</td>
                </tr>

                <!-- Rank = 8 -->
                <tr><td rowspan="4">Rank = 8</td><td>FL + LoRA </td><td class="underline">92.80 <span class="pm">± 0.24</span></td><td class="underline">90.47 <span class="pm">± 0.53</span></td><td>60.96 <span class="pm">± 1.47</span></td><td class="underline">70.44 <span class="pm">± 0.28</span></td><td class="underline">67.33 <span class="pm">± 0.18</span></td><td>43.90 <span class="pm">± 1.08</span></td><td>1.99B</td></tr>
                <tr><td>FFA-LoRA </td><td>87.20 <span class="pm">± 0.57</span></td><td>77.44 <span class="pm">± 1.28</span></td><td>40.88 <span class="pm">± 1.04</span></td><td>67.00 <span class="pm">± 0.67</span></td><td>61.27 <span class="pm">± 0.71</span></td><td>37.34 <span class="pm">± 0.30</span></td><td>0.991B</td></tr>
                <tr><td>FlexLoRA </td><td class="bold">93.35 <span class="pm">± 0.24</span></td><td class="bold">92.14 <span class="pm">± 0.25</span></td><td class="underline">69.84 <span class="pm">± 0.65</span></td><td class="bold">70.59 <span class="pm">± 0.22</span></td><td class="bold">68.10 <span class="pm">± 0.38</span></td><td class="bold">60.41 <span class="pm">± 1.54</span></td><td>1.99B</td></tr>
                <tr><td>Ours </td><td class="underline">93.24 <span class="pm">± 0.27</span></td><td class="underline">91.61 <span class="pm">± 0.39</span></td><td class="bold">70.13 <span class="pm">± 1.22</span></td><td>70.26 <span class="pm">± 0.21</span></td><td>67.12 <span class="pm">± 0.22</span></td><td class="underline">54.50 <span class="pm">± 1.44</span></td><td>1.31B</td></tr>

                <!-- Rank = 4 -->
                <tr><td rowspan="4">Rank = 4</td><td>FL + LoRA </td><td class="underline">92.86 <span class="pm">± 0.08</span></td><td>88.11 <span class="pm">± 0.88</span></td><td>54.99 <span class="pm">± 0.59</span></td><td>70.33 <span class="pm">± 0.12</span></td><td class="underline">67.29 <span class="pm">± 0.19</span></td><td>43.12 <span class="pm">± 2.67</span></td><td>0.991B</td></tr>
                <tr><td>FFA-LoRA </td><td>86.90 <span class="pm">± 1.14</span></td><td>76.38 <span class="pm">± 0.61</span></td><td>37.63 <span class="pm">± 0.80</span></td><td>67.75 <span class="pm">± 0.45</span></td><td>61.25 <span class="pm">± 0.26</span></td><td>36.04 <span class="pm">± 0.80</span></td><td>0.497B</td></tr>
                <tr><td>FlexLoRA </td><td>92.71 <span class="pm">± 0.31</span></td><td class="underline">90.53 <span class="pm">± 0.70</span></td><td class="underline">57.38 <span class="pm">± 1.30</span></td><td>70.05 <span class="pm">± 0.14</span></td><td class="bold">68.00 <span class="pm">± 0.33</span></td><td class="underline">50.50 <span class="pm">± 2.09</span></td><td>0.991B</td></tr>
                <tr><td>Ours </td><td class="bold">93.22 <span class="pm">± 0.24</span></td><td class="bold">91.43 <span class="pm">± 0.63</span></td><td class="bold">69.63 <span class="pm">± 1.52</span></td><td class="bold">70.28 <span class="pm">± 0.32</span></td><td>67.12 <span class="pm">± 0.60</span></td><td class="bold">53.04 <span class="pm">± 1.68</span></td><td>0.888B</td></tr>

                <!-- Rank = 2 -->
                <tr><td rowspan="4">Rank = 2</td><td>FL + LoRA </td><td>91.97 <span class="pm">± 0.43</span></td><td>85.59 <span class="pm">± 1.13</span></td><td>49.08 <span class="pm">± 0.56</span></td><td class="bold">70.14 <span class="pm">± 0.13</span></td><td>65.40 <span class="pm">± 0.31</span></td><td>39.07 <span class="pm">± 2.23</span></td><td>0.497B</td></tr>
                <tr><td>FFA-LoRA </td><td>84.65 <span class="pm">± 1.05</span></td><td>73.44 <span class="pm">± 0.88</span></td><td>34.44 <span class="pm">± 2.15</span></td><td>68.12 <span class="pm">± 0.47</span></td><td>61.57 <span class="pm">± 0.38</span></td><td>36.65 <span class="pm">± 0.52</span></td><td>0.249B</td></tr>
                <tr><td>FlexLoRA </td><td class="underline">92.22 <span class="pm">± 0.50</span></td><td>87.31 <span class="pm">± 0.27</span></td><td class="underline">55.24 <span class="pm">± 2.19</span></td><td>70.03 <span class="pm">± 0.31</span></td><td>66.17 <span class="pm">± 1.70</span></td><td>48.23 <span class="pm">± 1.73</span></td><td>0.497B</td></tr>
                <tr><td>Ours </td><td class="bold">93.10 <span class="pm">± 0.07</span></td><td class="bold">92.02 <span class="pm">± 0.36</span></td><td class="bold">69.40 <span class="pm">± 0.48</span></td><td class="underline">70.12 <span class="pm">± 0.18</span></td><td class="bold">67.02 <span class="pm">± 0.26</span></td><td class="bold">52.99 <span class="pm">± 2.56</span></td><td>0.528B</td></tr>

                <!-- Rank = 1 -->
                <tr><td rowspan="4">Rank = 1</td><td>FL + LoRA </td><td class="underline">90.61 <span class="pm">± 0.10</span></td><td class="underline">82.24 <span class="pm">± 1.68</span></td><td class="underline">45.78 <span class="pm">± 1.04</span></td><td>69.40 <span class="pm">± 0.33</span></td><td class="underline">63.16 <span class="pm">± 0.53</span></td><td class="underline">36.58 <span class="pm">± 0.98</span></td><td>0.249B</td></tr>
                <tr><td>FFA-LoRA </td><td>82.51 <span class="pm">± 0.53</span></td><td>72.96 <span class="pm">± 0.54</span></td><td>33.68 <span class="pm">± 0.20</span></td><td>67.73 <span class="pm">± 0.30</span></td><td>61.35 <span class="pm">± 0.22</span></td><td>34.44 <span class="pm">± 0.68</span></td><td>0.124B</td></tr>
                <tr><td>FlexLoRA </td><td>90.40 <span class="pm">± 0.54</span></td><td>82.20 <span class="pm">± 0.74</span></td><td>42.75 <span class="pm">± 0.89</span></td><td class="underline">69.53 <span class="pm">± 0.25</span></td><td>62.98 <span class="pm">± 1.12</span></td><td>35.54 <span class="pm">± 0.68</span></td><td>0.249B</td></tr>
                <tr><td>Ours </td><td class="bold">93.21 <span class="pm">± 0.13</span></td><td class="bold">91.87 <span class="pm">± 0.33</span></td><td class="bold">68.88 <span class="pm">± 1.15</span></td><td class="bold">70.31 <span class="pm">± 0.24</span></td><td class="bold">66.95 <span class="pm">± 0.07</span></td><td class="bold">54.84 <span class="pm">± 1.15</span></td><td>0.270B</td></tr>
              </tbody>
            </table>
          </div>      
          
          <p style="font-size: 0.85em;">
            <strong>Table:</strong> Results with RoBERTa-base on BANKING77 and 20 Newsgroups datasets. Smaller \( \alpha \) for \( Dir(\alpha) \) implies that the simulated setting is more heterogeneous. 
            The best results on each dataset are shown in <span class="bold">bold</span> and second best in <span class="underline">underline</span>. The metric used here is accuracy (%).
            <sup>*</sup>This column reports the total number of uploaded parameters, averaged across rows.
          </p>
          
        </div>
      </div>
    </div>
  </div>
</section>

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{koo2024robustefficientfederatedlowrank,
        title={Towards Robust and Efficient Federated Low-Rank Adaptation with Heterogeneous Clients}, 
        author={Jabin Koo and Minwoo Jang and Jungseul Ok},
        year={2024},
        eprint={2410.22815},
        archivePrefix={arXiv},
        primaryClass={cs.LG},
        url={https://arxiv.org/abs/2410.22815}, 
  }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<script type="text/javascript">
  var sc_project=13138827; 
  var sc_invisible=1; 
  var sc_security="133cd9a3"; 
  </script>
  <script type="text/javascript"
  src="https://www.statcounter.com/counter/counter.js" async></script>
  <noscript><div class="statcounter"><a title="Web Analytics Made Easy -
  Statcounter" href="https://statcounter.com/" target="_blank"><img
  class="statcounter" src="https://c.statcounter.com/13138827/0/133cd9a3/1/"
  alt="Web Analytics Made Easy - Statcounter"
  referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>

  <!-- End of Statcounter Code -->

  </body>
  </html>
